% !TEX root =main.tex
\section{Survey of Related Work}\label{Survey-of-Related-Work}
\subsection{Time-lock Puzzles}

The idea to send information into the \emph{future}, i.e.
time-lock puzzle/encryption, was first put forth by Timothy C. May. A time-lock puzzle allows a party to encrypt a message such that it cannot be decrypted  until a certain amount of time has passed. In general,  a  time-lock scheme should allow    generating (and verifying) a puzzle to take less time than solving it. The time-lock puzzle scheme that May proposed lies on a trusted agent. Later on, Rivest \textit{et al} \cite{Rivest:1996:TPT:888615} propose a protocol that does not require a trusted agent, and is secure against a receiver
who may have access to many  computation resources that can be run in parallel. It is based on Blum-Blum-Shub pseudorandom number generator that relies on modular repeated squaring, believed to be sequential. The scheme in \cite{Rivest:1996:TPT:888615} allows (only) the puzzle creator to  verify the correctness of the puzzle solution using a secret key and the original secret message.  This scheme has been the core of (almost) all later time-lock puzzles schemes that supports the encapsulation of an arbitrary message. Later on, \cite{BonehN00,DBLP:conf/fc/GarayJ02} proposed timed commitment schemes that offer more security properties, in the sense that they   allow a puzzle generator to prove (in Zero-knowledge) to a puzzle solver that the correct solution (e.g. a signature of a public document) will be recovered after a certain time, before the solver starts solving the puzzle. These schemes are more complex, due to the use of zero-knowledge proofs, and less efficient than \cite{Rivest:1996:TPT:888615}.    Very recently, \cite{MalavoltaT19,BrakerskiDGM19}  propose protocols for homomorphic time-lock puzzles, where an arbitrary function can be run over puzzles before they are solved. They mainly use  fully homomorphic encryption and   the RSA puzzle, proposed in \cite{BrakerskiDGM19}, in a nutshell. In these protocols, all puzzles have an identical time parameter, and their solutions are supposed to be discovered at the same time. The main difference between the two protocols is the security assumption they rely on (i.e. the former uses a non-standard assumption while the latter relies on a standard one). Since both schemes use a generic fully homomorphic encryption, it is not hard to make them publicly verifiable. However, they  are only of theoretical interest as in practice they impose  high computation and communication costs, due to the use of fully homomorphic encryptions.


%Furthermore, \cite{KarameC10} proposes a privately verifiable puzzle scheme that has up to $12\times$  lower cost than \cite{Rivest:1996:TPT:888615} in puzzle generation and verification phases. However,  it relies on  a new and non-standard assumption (i.e. computationally infeasible to compute a small private exponent when a public exponent is much larger than the RSA modulus).



We also cover two related but different notions, pricing puzzles and verifiable delay functions. 

\noindent\textbf{\textit{Pricing Puzzles.}} Also known as \emph{client puzzles}. It was first put forth by Dwork \textit{et al.} \cite{DworkN92} who defined it as a function that requires a certain amount of computation resources to solve a puzzle.  In general, the pricing puzzles are based on either hash inversion problems or number theoretic. In the former category, 
a puzzle generator  generates a puzzle as: $h= \mathtt{H}(m||r)$, where $\mathtt{H}$ is a hash function, $m$ is a public value and $r$ is a random value of a fixed size. Given $h, \mathtt{H}$ and $m$, the solver must find $r$ such that the above equation holds. The size of $r$ is picked in such a way that the expected time to find the solution is fixed (however it does not rule out finding the solution on the first attempt). The above hash-based scheme allows a solver to find a solution faster if it has more computational power resources  running in parallel. The application area of such a puzzle includes  defending against denial-of-service (DoS)  attacks, reaching a consensus in cryptocurrencies, etc. A  variant  of such a puzzle uses iterative hashing; for instance, to generate a set of puzzles  in the case where the solver receives a service proportional to the number of puzzles it solves \cite{groza2006chained},  or to generate password puzzle to mitigate DoS attacks \cite{Ma05}. However, the iterative hashing schemes are partially parallelizable, in the sense that each single invocation of the hash function can be run in parallel. Later on,  \cite{MahmoodyMV11} investigates the possibility of constructing (time-lock) puzzles in the random oracle model.  Their main result was negative, that rules out time-lock puzzles that require more parallel time to solve than the total work required to generate.  Also \cite{MahmoodyMV11} proposes an iterative hash-based mechanism (very similar to \cite{Ma05}) that allows a puzzle generator to generate a puzzle with $n$ parallel queries to the random oracle, but the solver needs $n$ rounds of serial queries. Nevertheless, this scheme is also partially parallelizable, as each instance of the puzzle can be solved in parallel. Note that the above hash-based puzzle schemes would have very limited applications if they are used directly to  encapsulate a message: $m'$ of arbitrary size. The reason is that, in these schemes,  the solution  size: $|r|$ plays a vital role in (adjusting) the  time taken to solve the puzzle. If the solution size becomes bigger, as a result of combining $r$ with $m'$, i.e. $r \odot m'$, then it would take longer to find the solution. This means  the puzzles can be used only in the cases where  the time required to find a solution is long enough, and is a function of $|r \odot m'|$, which seriously restricts its application. Researchers also propose non-parallelizable pricing puzzles based on number theoretic \cite{WatersJHF04,KuppusamyRSBN12,KarameC10} whose main application is to resist DoS attacks. These schemes  have a more efficient verification mechanism than the one proposed in \cite{Rivest:1996:TPT:888615}. But, they are only privately verifiable and not designed to encapsulate an arbitrary message. 





\noindent\textbf{\textit{Verifiable Delay Function (VDF).}} Allows a prover to provide a publicly verifiable proof stating  it has performed  a pre-determined number of sequential computations. It has many applications, e.g. in decentralised systems to extract  trustworthy public randomness from a blockchain. VDF was first formalised by Boneh \textit{et al} in \cite{BonehBBF18} who proposed several VDF constructions based on SNARKs along with either  incrementally verifiable computation or injective polynomials, or based on time-lock puzzles, where  the SNARKs based approaches require a trusted setup.  Later on,  \cite{Wesolowski19} improved the previous VDF's  from different perspectives and proposed a scheme  based on RSA time-lock encryption, in the random oracle model. To date, this protocol is the most efficient VDF.  It also supports batch verification, such that given a single proof a verifier can efficiently check the validity of multiple outputs of the verifiable delay function. As discussed above, (most of) VDF schemes are built  upon time-lock puzzles, however the converse is not necessarily the case, as VDF's are not designed to encapsulate an  arbitrary private message, and they take a public message as input while time-lock puzzles are designed to conceal a private input message. 



\subsection{Proof of Storage}\label{related-work-PoR}

\subsubsection{Traditional Proof of Storage}

Proof of storage (PoS) is a cryptographic protocol that allows a client to  efficiently verify the integrity or availability of its  data stored in a remote server, not necessarily trusted \cite{DBLP:conf/cai/Kamara13}. PoS  can be categorised into two broad classes:   Proofs of Retrievability (PoR) and Proofs of Data Possession (PDP). The main difference between the two categories is the level of security assurance provided. PoR schemes guarantee that the server maintains knowledge of \emph{all} of the client's outsourced data, while  PDP protocols only ensure that the server is storing \emph{most} of the client data. From a technical point of view,  the main difference in most prior PDP and PoR constructions is that PoR schemes store a redundant encoding of the client data on the server by employing an error-correcting code, e.g. Reed-Solomon codes, while such encoding is not utilised in  PDP schemes. Hence, PoR schemes provide stronger security guarantees compared to PDP at the  cost of additional storage space and encoding/decoding, (for more details see \cite{kupcu2010efficient,DBLP:conf/ccs/ShiSP13,Cash:2017:DPR:3038037.3038087}). Furthermore, each PoR and PDP scheme can be also grouped into two categories; namely, publicly and privately verifiable. In a publicly verifiable scheme, everyone without knowing a secret can verify  proof, whereas a verifier in a privately verifiable scheme requires the knowledge of a secret. 
%As highlighted in \cite{kupcu2010efficient,DBLP:conf/ccs/ShiSP13,Cash:2017:DPR:3038037.3038087}, the main difference between PoR and PDP is the level of the security they achieve; 

The notion of PoR first was introduced and defined in \cite{DBLP:conf/ccs/JuelsK07}, where the authors designed a protocol that utilises  random sentinels, symmetric key encryption,  error-correcting code and pseudorandom permutation. In this protocol, a client in the setup phase, applies error-correcting code to every file block and encrypts each encoded block. Then, it computes a set of random sentinels and appends them to the encrypted file. It  permutes all values (i.e. sentinels and encrypted file blocks) and sends them to the cloud  server.  To check if  the server has retained the file, the client specifies  random positions of some sentinels in the encoded file and asks
the server to return those sentinel values. Next, the client checks if it gets the sentinels it asked for. In this scheme, the security holds, as the server cannot feasibly distinguish between sentinels and the actual file blocks and the sentinels have been distributed uniformly among the file blocks. But, as individual sentinel is only one-time verifiable, there is an upper bound on the number of verifications performed by the client, and when reached, the client has to re-encode the file.  To overcome the issues related to the bounded number of verifications, the authors have also suggested that sentinels can be replaced with  MAC on every file block, or a Merkle tree constructed on the file blocks. This protocol is computationally efficient and its communication cost is linear with the number of challenges sent\footnote{It is not hard to make the communication cost of this scheme constant; for instance, by letting the server order the challenged sentinels, concatenate them and send a hash of the concatenated value to the client.}. The sentinel and MAC-based schemes above are privately verifiable while the one uses Merkle tree supports public verifiability. However, the publicly verifiable Merkle tree based approach has a communication cost logarithmic with the file size and the prover has to send a set of file blocks to the verifier that yields a high communication cost. 


Later on, \cite{DBLP:conf/asiacrypt/ShachamW08} improves the previous sentinel-based scheme and definition, and proposes two PoR protocols (with an unlimited number of verifications), one utilises  MAC and the other one relies on BLS signatures. In particular, a client at setup phase, encodes its file blocks using error-correcting code and then for each encoded file block it generates a  tag that can be either a MAC or BLS signature of that block. In the verification phase, it specifies a set of random indices corresponding to the file's blocks; and accordingly the server sends a proof to it. These two  schemes have a low communication cost, as due   to the homomorphic property of the tags,  the prover can aggregate  proofs  into a single authenticator value and no file blocks  are sent to the prover. Also, the protocol based on MAC supports efficient private verification. Nevertheless, the one that uses BLS signature supports public verifiability at the cost of public key operations and it is far less efficient than the MAC-based one. Within the  last decade, researchers have extended PoR protocols from several perspectives, e.g. those that support:  efficient update \cite{DBLP:conf/ccs/ShiSP13}, delegation of file pre-processing \cite{ArmknechtBBK16}, or delegation of verification \cite{armknecht2014outsourced}.  


On the other hand, PDP was first introduced in \cite{DBLP:conf/ccs/AtenieseBCHKPS07}. PDP protocols focus only on verifying the  integrity of outsourced data. In this setting,  clients can ensure that a certain percentage of data blocks are available. The authors in \cite{DBLP:conf/ccs/AtenieseBCHKPS07} propose two schemes, for public and private verification both of which utilise RSA-based homomorphic verifiable tags  generated for each file block and use the spot-checking techniques (similar to \cite{DBLP:conf/asiacrypt/ShachamW08}) to check a random subset of a file's blocks. In both schemes, the proof size is constant, while the verification cost is high as it requires  many  exponentiations over an RSA ring. Later on, an  efficient and scalable  PDP scheme that supports a limited number of verifications is proposed in \cite{AteniesePMT08}. The scheme supports only privately verifiable PDP, and is based on a combination of pre-computation technique and symmetric key primitives.  Ever since, researcher proposed different variants of PDP, e.g.  dynamic PDP \cite{ErwayKPT09}, multi-replica PDP\cite{DistributedPDP}, keyword-based PDP \cite{SenguptaR18}.  

\
Thus,  (a) in publicly verifiable PoS it is assumed the verifier is fully trusted with the verification correctness, (b) these schemes either have a very high verification cost when the proof size is constant (when signature-based tags are used) or have a communication cost logarithmic with the entire file size if their verification is efficient (when a Merkle tree is utilised),  and (c) the privately verifiable proofs can have a constant proof size and efficient verification algorithm, but the data owner has to perform the verification. 



\subsubsection {Outsourced PoR}\label{Outsourced-PoS}

Recently,  \cite{armknecht2014outsourced,xu2016lightweight} propose \emph{outsourced} PoR protocols that allow  clients to outsource the   verification to a third party auditor not necessarily  trusted. The scheme in \cite{armknecht2014outsourced} uses MAC-based tags, zero-knowledge proofs and error-correcting codes. At a high level, the protocol works as follows. At the setup,   the client  encodes its data using error-correcting codes, generates MAC on every file block, and stores the encoded file and tags on the server.  Then, the auditor downloads the encoded file, generates another set of MAC's on the file blocks. It uploads the MAC's to the cloud. Also, the auditor proves to the client in zero-knowledge that it has created each MAC correctly. To do that, it uses a non-interactive zero-knowledge proof in the random oracle model. If the client accepts all zero-knowledge proofs, it sings every proof and sends the signatures back to the auditor. In the verification phase, the auditor sends two sets of random challenges extracted from a blockchain. Upon receiving the challenges, the server provides two separate proofs, one for the auditor and the other one for the client. The auditor verifies the proof generated for it and  locally stores the clients' proofs. In the case where the auditor's proof is not accepted, an honest auditor would inform the client who will come online and checks both its proofs and the auditors' proof. The scheme provides two layers of verification to the client: \textit{CheckLog} and \textit{ProveLog}.  \textit{CheckLog}  is more efficient than the other one, as the auditor sends  much fewer challenges to the server to generate the client's proof for each verification. In the case where the client's check fails, the client will assume that either the server or auditor has acted maliciously, and to pinpoint the malicious party, it needs to proceed to \textit{ProveLog} where allows the client to audit the auditor. In this verification, the auditor must reveal its secret keys with which the client checks all the proofs provided by the server to the auditor for the entire period that the client was offline. In this protocol, the verification phase is efficient (due to the use of MACs). In particular, the verification's computation cost for the auditor and client is linear with the number of challenges and their communication cost is constant in the file size.  

Although the protocol in  \cite{armknecht2014outsourced}  is appealing, it has  several shortcomings: (a) auditor can get a free ride: in the case where a known highly reputable  server, e.g. Google or Amazon, always generates   accepting proofs for both client and auditor, an economically rational auditor can skip performing its part (e.g. to save computation cost) and get still paid by the client. This deviation from the protocol can never be detected by the client in the protocol.  (b) no guarantee for a real-time detection/notification: the client may not be notified in the real-time about the data unavailability if the auditor is malicious; therefore, this scheme is not suitable for the case where a client's involvement for the data extraction is  immediately needed once a misbehaviour is detected, e.g., in the case of hardware depreciation. (c) a high cost of auditor onboarding:  revoking an auditor and onboarding a new one, imposes a high cost on the client and new auditor, as new auditor need to re-run the setup phase (that includes data downloading, zero-knowledge proofs) and the client needs to verify and sign the zero-knowledge proofs. (d) \textit{ProveLog} costs even an honest auditor: the only way for the client to ensure the auditor has fully followed the protocol, is to run \textit{ProveLog} that requires the auditor to reveal its secret values. After that,  an honest auditor has to re-run the computationally expensive setup  again. Since the auditor is not trusted and no guarantee that it alerts the client as soon as the server's misbehaviour is detected, its involvement in the protocol seems unnecessary; and the whole scheme can be replaced with a much simpler one: the client, similar to a standard privately verifiable PoR, encodes its data and stores it in the server. Then, for each verification time, the server gets the random challenges from the blockchain and publishes  proofs in a bulletin board (to timestamp it). When the client comes online, it checks the proofs and accordingly takes the required action, e.g. pays the server, tries to recover/ the file.

Xu et al. in \cite{xu2016lightweight} propose a publicly verifiable PoR to improve the computation cost at setup. The scheme uses   BLS signatures-like tags, polynomial arithmetics, and polynomial commitments; unlike previous schemes,  each tag has a more complex structure.  In this protocol, an auditor is assumed to be fully trusted during the verifications. Later on, however, when  it is revoked by the client it may become malicious, i.e. may collude with the server and reveal its secret. This scheme allows a client to update its tags when a new auditor joins. To do that, the client needs to download all the blocks' tags, refresh them locally and uploaded the fresh tags to the server. The verification for auditor and client involves public key operations and is more expensive than the one in \cite{armknecht2014outsourced}. The use of an auditor's revocation remains unclear in the paper, as  auditors are assumed to be honest in this protocol. A delegatable PDP is proposed in \cite{ShenT11} to ensure only authorised parties can verify the integrity of remote data. However, the delegated party in this scheme is fully trusted with the correctness of verification it performs. The scheme uses authenticator tags  similar to   BLS signatures based ones.


Very recently, proof of ``storage-time'' has been proposed in \cite{Storage-Time}. The paper proposes two protocols: basic PoSt and compact PoSt. At a high level, they offer the guarantee to a client that its data has been available on a remote storage server for a fixed time period: $T$.  The idea is that a client uploads to a server its data once, then  the server generates proofs of storage (e.g. PoR) periodically, collects them and sends the collection after the time $T$ to a verifier (i.e. client in basic PoSt or third-party in compact PoSt) who can check the proof. The first protocol, basic PoSt, uses Merkle tree-based PoR and VDF. In this protocol, The client precomputes a set of metadata (tags, challenges and their related proofs). It  sends  the file, metadata and a single challenge to the server. The server uses the  challenge to generate a PoR. It feeds the hash of the PoR to VDF to generate an output. It considers the hash of VDF's output as the next challenge from which  the next PoR is generated. This process goes on until all $z$ PoR's are generated. It sends all PoR proofs along with the proofs proving the correctness of VDF's outputs to a verifier. Given the two sets of proofs, a verifier can check their correctness and ultimately conclude that the file was retrievable within the time period. The scheme is publicly verifiable. However, the verification's computation cost is significant.  As, it requires  the validator to validate the correctness of VDF's outputs, that imposes a high cost. In total (for $z$ PoR proofs) it involves at least $3z$ modular exponentiations even if it uses  the fastest VDF, proposed in \cite{Wesolowski19}. Such costs are missed out and not taken into account in the paper. Moreover, the use of the Merkle tree introduces high communication cost, as well. The authors suggest a smart contract can play the validator's role. However, we argue that this would impose a significant financial cost to the contract and users due to very high computation and communication costs stem from the verification and prove algorithms. The second protocol, compact PoSt, allows the server to combine PoR proofs that ultimately reduces the communication cost. The protocol's design is very similar to the previous one, but it replaces the Merkle tree approach with simply hashing the entire file and replaces the VDF with a trapdoor delay function (TDF) that requires a secret to generate/verify the delay function's output. Therefore, this protocol is only privately verifiable. The paper claims that the verification in this protocol can be performed by a trusted third-party, e.g. smart contract. Nevertheless, we argue that this is not the case. As,  the scheme requires a set of secrets (e.g. challenges) to perform the verification, while a smart contract does not maintain a private state. If the challenges are given to the contract, then the server can read the contract and compute all proofs in one go (which violates the security requirement set out in the paper). Moreover, the same security issue would arise in the case where the verification is delegated to a third-party auditor who may collude with the server (as the auditor can send all challenges to the server at once). Therefore, the only secure option, in the compact PoSt, is that  the verifier performs the validation itself (i.e. private verification/validation).  Note that   all the above outsourced PoR schemes \cite{armknecht2014outsourced,xu2016lightweight,Storage-Time}  assume the client behaves honestly towards the server. Otherwise, a malicious client can generate the tags in a way that  makes an honest server generate invalid proofs. 







Hence, the existing outsourced PoR protocols either suffer from several security issues and guarantee no real-time detection or have to fully trust verifiers with the verification correctness, or are very inefficient. 


 \subsubsection{Distributed PoR using a (Tailored) Blockchain}
 
 
Distributed  PoR allows  data to be distributed to numerous storage servers to achieve robustness, and address a single point of failure issue. Permacoin \cite{MillerPermacoin} is one of those  that distribute data among  customised blockchain nodes and  repurposes  mining resources of Bitcoin blockchain miners. In Permacoin, each miner needs to prove that it has a portion of the file and to do so it provides  proof of data retrievability verified by other miners. The miner, in this scheme needs to invest on both computation resources  and storage resources (to generate proof of retrievability). The protocol uses a Merkle tree built on top of file blocks to support publicly verifiable PoR. The mining procedure in this protocol is based on iterative hashing. In Permacoin, in each verification, the prover has to send both challenged file blocks and  the proof path in the tree, that imposes communication cost logarithmic to the size of the \emph{entire original file}.  In this scheme, an accepting proof only indicates a portion of the data holding by that miner (prover) is retrievable. Thus, for a data owner to have a guarantee that the entire data is retrievable, it  has to either wait for a long period of time (depending on the file size and the number of miners) or hope that there are enough active miners in each epoch.  Also, since the miners are both verifiers and provers (storage providers), it is assumed that the storage providers are economically rational (which is weaker than a malicious one). %Furthermore, the scheme suffers a vital security issue that allows a misbhaving miner to anticipate the challenge  file blocks for next rounds, if it is a miner for  consecutive blocks.  There are two protocols proposed in \cite{}, basic and advanced. The main difference between the two is that in the latter one the miner also signs a random index  and feeds the signature to the  procedure via which the next random index is created. For the sake of simplicity, we consider the basic protocol,  we will discus the same security issue is inherited by the advanced one as well. In particular, to compute a random index $r_{i}$, a miner computes $r_{i}=H(puz||pk||i||s)$ where $s$ is picked by the miner and $puz$ contains the header of previous block. Value $puz$ is the only source of randomness and would be unpredictable to the adversary if the previous block is proposed by an honest miner who includes a random $s$ in the block. However, if the adversary is chosen as a miner for $m$ consecutive blocks, then it can predict the random indices for every next $m-1$  blocks. Therefore, for a certain time period the POR (and Permacoin) security  is violated (e.g. can pre-compute the proof) and the availability of even the partial file is not guaranteed for a time period. Such issue does not hold in Bitcoin (with a high probability) as the randomness of each block is determined by a solution (nonce) of proof of work, not used in Permacoin. 





Filecoin and KopperCoin in \cite{Filecoin,KoppBK16} respectively, offer  similar features, i.e. repurposing Bitcoin and supporting distributed PoR.  However, Filecoin, in addition to a Merkle tree, utilises  generic zero-knowledge proofs (i.e. zk-SNARKS) that result in a high  overall computation  cost and requires a trusted party to generate the system parameters. Filecoin uses proof of work as well as PoR. On the other hand, 
 KopperCoin uses BLS signature-based publicly verifiable PoR that has a constant communication cost but has a high computation cost. Unlike Filecoin, KopperCoin does not use a PoW. 

In the same line of research, \cite{KoppMHKB17} proposes a customised blockchain  that supports distributed PoR as well as  preserving the privacy of on-chain payments between storage users and providers. It however is computationally expensive as it uses publicly verifiable  tags based on  BLS signatures for PoR and ring signatures for the privacy-preserving payments.  Likewise, \cite{RujRBK18} proposes a high-level distributed PoR framework whose aim is efficient utilization of  storage resources offered by storage providers. It also uses BLS signatures and a Merkle tree along with a smart contract  for payments. Similarly, \cite{XueX0B18,XueXB19} propose  schemes that allow a user to store its encrypted data in the blockchain. The scheme uses a Merkle tree  for PoR   and smart contract to transfer fees to the blockchain nodes storing  the data. In these two schemes, the data owner  is the party who sends random challenges  to  storage nodes, so it has to be  online when  verification is needed. Storj \cite{storj14} also falls in this category where there is a tailored blockchain comprising a set of storage nodes that store a part of data and provide  proofs when they are challenged. In Storj, there are trusted nodes, Satellite, who do the verifications on behalf of the clients.  The scheme uses a Merkle tree-based   PoR. 

So,  existing distributed PoR protocols have either a large proof size or  high verification cost. Also,  they do not guarantee  that the \emph{entire} file is retrievable in  \emph{real-time}. 



 \subsubsection{Verifying Remote (off-chain) data via a Blockchain}
 To relax the assumption that an auditor is fully trusted with the correctness of verification in the publicly verifiable PoR schemes while storing data off-chain, e.g. in a cloud, researchers proposed numerous protocols, e.g. \cite{RennerMK18,HaoXWJW18,ZhangDLZ18,Audita18,blockchain-data-audit-18,sia14}, that delegate the verification procedure  to blockchain nodes.  The high-level framework  proposed in \cite{RennerMK18} requires only a hash of the entire file is stored in a smart contract, where when later on the user access the file, it computes the hash of the file and compares it with the one stored in the contract. In this scheme, the entire file has to be accessed by the user for each verification which is what exactly PoR schemes avoid doing. The protocol  in \cite{HaoXWJW18} uses BLS signature-based tags and Merkle tree where the tags are broadcast to all blockchain nodes. However, surprisingly, the protocol assumes the cloud server is fully trusted and stores data safely, which raises the question that \textit{``why is a data verification needed in the first place?''}. In this protocol, the tags are generated by the cloud who sends them to the nodes. The tags are never checked against the outsourced data.  So, the only security guarantee \cite{HaoXWJW18} offers is the immutability of tags. The high-level scheme proposed in \cite{ZhangDLZ18} allows a client to pay the storage server in a fair manner. The scheme uses a blockchain for payment    and Merkle tree for PoR. In this protocol, the client has to be online and send random challenges to the server for every verification. 
 Audita  \cite{Audita18} utilises an augmented blockchain and RSA signature-based  PDP  \cite{DBLP:conf/ccs/AtenieseBCHKPS07} to achieve its goal. In this scheme miners, for each epoch, pick a dealer who sends a challenge to the storage node(s) to get  proofs of data possession. Similar to Permacoin \cite{MillerPermacoin}, Audita substitutes proof of work with PDP, so if a proof is accepted a new block is added to the chain. But, in Audita every miner carries out  expensive public key based verifications.  The protocol proposed in \cite{blockchain-data-audit-18},   similar to \cite{armknecht2014outsourced}, uses a third-party auditor. It mainly utilises, a smart contract and  BLS signature-based tags. In this protocol, unlike \cite{armknecht2014outsourced},  when the auditor raises a dispute during the verification it calls a smart contract who performs the verification again to detect a misbehaving party, i.e. the server or auditor.  The protocol is computationally more expensive than \cite{armknecht2014outsourced} and inherits the same  issues, i.e. issues (a-c) stated above.  Sia \cite{sia14} is a mechanism in which a data owner  distributes  its data among off-chain storage servers who periodically provide a PoR  to a smart contract signed between the data owner and the servers. Each server gets paid if its proof is accepted by the contract. In this scheme, a Merkle tree-based PoR is used. 
 
As evident,  the schemes designed for  blockchain-based verification of data stored in a storage server either require clients to access  whole outsourced data for  every verification, or impose a high communication/computation cost, or  clients have to be online for each verification. 

 \subsubsection{Fair Exchange of Digital Services}
 
 Campanelli \textit{et al.}  \cite{CampanelliGGN17} propose a scheme that allows different parties to exchange digital services (and goods) over Bitcoin blockchain. This scheme, for instance in PoR context, allows the storage provider to get paid  if and only if the data owner receives an accepting proof. It has two variants, publicly and  privately verifiable both of which use a smart contract.   In addition, in the privately verifiable variant, a MAC-based tags and generic secure multi-party computation are used, e.g. Yao's garbled circuit \cite{Yao82b}, while in the publicly verifiable one  BLS signature tags and zk-SNARK are utilised.  Nevertheless, this scheme assumes that either the client is  available and online when PoR is provided (in privately verifiable variant) or the third party, acting on a client's behalf, performs PoR verification honestly (in publicly verifiable one). 

%BonehBBF18,Wesolowski19











